# ğŸ¯ Multi-Domain Aspect-Based Sentiment Analysis: STL vs MTL Comparison

Dá»± Ã¡n so sÃ¡nh hiá»‡u quáº£ cá»§a **Single-Task Learning (STL)** vÃ  **Multi-Task Learning (MTL)** trong phÃ¢n tÃ­ch cáº£m xÃºc Ä‘a khÃ­a cáº¡nh (Aspect-Based Sentiment Analysis) trÃªn nhiá»u domain báº±ng PhoBERT.

## ğŸ“Š Tá»•ng quan

Aspect-Based Sentiment Analysis (ABSA) lÃ  nhiá»‡m vá»¥ phÃ¢n tÃ­ch cáº£m xÃºc theo tá»«ng khÃ­a cáº¡nh cá»¥ thá»ƒ cá»§a má»™t sáº£n pháº©m/dá»‹ch vá»¥. VÃ­ dá»¥, vá»›i review "Äá»“ Äƒn ngon nhÆ°ng phá»¥c vá»¥ cháº­m", ta cáº§n phÃ¢n tÃ­ch:
- **FOOD#QUALITY**: Positive (ngon)
- **SERVICE#GENERAL**: Negative (cháº­m)

## ğŸ¯ Má»¥c tiÃªu nghiÃªn cá»©u

So sÃ¡nh 2 phÆ°Æ¡ng phÃ¡p:

### ğŸ”¹ Single-Task Learning (STL)
**ğŸ“ Vá»‹ trÃ­ trong code**: Lines 82-97 (`STLModel`) vÃ  Lines 254-358 (`train_stl_models()`)

**ğŸ—ï¸ Kiáº¿n trÃºc**:
```python
class STLModel(nn.Module):
    def __init__(self, model_name, num_classes, dropout_rate=0.3):
        self.transformer = AutoModel.from_pretrained(model_name)
        self.dropout = nn.Dropout(dropout_rate)
        self.classifier = nn.Linear(self.transformer.config.hidden_size, num_classes)  # Má»˜T classifier duy nháº¥t
```

**ğŸ”„ Training Process**:
- **Má»—i aspect cÃ³ model hoÃ n toÃ n riÃªng biá»‡t**: `for aspect, data in domain_data.items()`
- **Train tuáº§n tá»±**: Model cho `texture` â†’ Model cho `smell` â†’ Model cho `price`...
- **Äá»™c láº­p hoÃ n toÃ n**: KhÃ´ng chia sáº» parameters giá»¯a cÃ¡c aspects

**âœ… Æ¯u Ä‘iá»ƒm**: 
- Táº­p trung cao cho tá»«ng aspect cá»¥ thá»ƒ
- KhÃ´ng bá»‹ nhiá»…u tá»« aspects khÃ¡c
- Dá»… debug vÃ  tune cho tá»«ng task

**âŒ NhÆ°á»£c Ä‘iá»ƒm**: 
- KhÃ´ng táº­n dá»¥ng Ä‘Æ°á»£c shared knowledge
- Tá»‘n nhiá»u memory vÃ  thá»i gian (N models cho N aspects)
- KhÃ´ng há»c Ä‘Æ°á»£c má»‘i quan há»‡ giá»¯a cÃ¡c aspects

### ğŸ”¹ Multi-Task Learning (MTL)  
**ğŸ“ Vá»‹ trÃ­ trong code**: Lines 100-117 (`MTLModel`) vÃ  Lines 370-525 (`train_mtl_model()`)

**ğŸ—ï¸ Kiáº¿n trÃºc**:
```python
class MTLModel(nn.Module):
    def __init__(self, model_name, aspect_classes, dropout_rate=0.3):
        # SHARED transformer encoder cho táº¥t cáº£ aspects
        self.transformer = AutoModel.from_pretrained(model_name)
        self.dropout = nn.Dropout(dropout_rate)
        
        # NHIá»€U task-specific classifiers
        self.classifiers = nn.ModuleDict()
        for aspect, num_classes in aspect_classes.items():
            self.classifiers[aspect] = nn.Linear(hidden_size, num_classes)
```

**ğŸ”„ Training Process**:
- **Má»™t model duy nháº¥t cho táº¥t cáº£ aspects**: `model = MTLModel(self.model_name, aspect_classes)`
- **Train Ä‘á»“ng thá»i**: Táº¥t cáº£ aspects Ä‘Æ°á»£c há»c cÃ¹ng lÃºc trong má»—i batch
- **Shared feature extraction**: `shared_features = self.dropout(outputs.pooler_output)`
- **Multi-task loss**: `total_batch_loss += aspect_loss` cho táº¥t cáº£ aspects

**âœ… Æ¯u Ä‘iá»ƒm**: 
- Táº­n dá»¥ng shared knowledge giá»¯a cÃ¡c aspects
- Hiá»‡u quáº£ memory (1 model thay vÃ¬ N models)
- Há»c Ä‘Æ°á»£c correlation giá»¯a cÃ¡c aspects
- Regularization effect tá»« multiple tasks

**âŒ NhÆ°á»£c Ä‘iá»ƒm**: 
- CÃ³ thá»ƒ bá»‹ nhiá»…u tá»« cÃ¡c task khÃ¡c
- KhÃ³ tune optimal cho tá»«ng aspect riÃªng láº»
- Task imbalance cÃ³ thá»ƒ áº£nh hÆ°á»Ÿng performance

## ğŸ“ Cáº¥u trÃºc dá»± Ã¡n

```
Lab10_1/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ full_data.csv                 # Dá»¯ liá»‡u má»¹ pháº©m (Cosmetic)
â”‚   â”œâ”€â”€ VLSP-Hotel/                   # Dá»¯ liá»‡u khÃ¡ch sáº¡n VLSP 2018
â”‚   â”‚   â”œâ”€â”€ 1-VLSP2018-SA-Hotel-train.csv
â”‚   â”‚   â”œâ”€â”€ 2-VLSP2018-SA-Hotel-dev.csv
â”‚   â”‚   â””â”€â”€ 3-VLSP2018-SA-Hotel-test.csv
â”‚   â””â”€â”€ VLSP-Restaurant/              # Dá»¯ liá»‡u nhÃ  hÃ ng VLSP 2018
â”‚       â”œâ”€â”€ 1-VLSP2018-SA-Restaurant-train.csv
â”‚       â”œâ”€â”€ 2-VLSP2018-SA-Restaurant-dev.csv
â”‚       â””â”€â”€ 3-VLSP2018-SA-Restaurant-test.csv
â”œâ”€â”€ nlp-lab-10 (1).ipynb            # Notebook chÃ­nh
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â””â”€â”€ .gitignore
```

## ğŸ”„ So sÃ¡nh Architecture STL vs MTL

### ğŸ“Š Sá»± khÃ¡c biá»‡t chÃ­nh trong code:

| Aspect | STL | MTL |
|--------|-----|-----|
| **Model Class** | `STLModel` | `MTLModel` |
| **Training Function** | `train_stl_models()` | `train_mtl_model()` |
| **Number of Models** | N models (1 per aspect) | 1 model (shared) |
| **Classifier Layer** | `nn.Linear(hidden_size, num_classes)` | `nn.ModuleDict()` vá»›i nhiá»u classifiers |
| **Forward Output** | `logits` (single tensor) | `logits` (dictionary cá»§a tensors) |
| **Training Loop** | `for aspect in domain_data.items()` | `for aspect in aspect_classes` |
| **Loss Calculation** | Äá»™c láº­p cho má»—i aspect | Cá»™ng dá»“n táº¥t cáº£ aspect losses |

### ğŸ—ï¸ Architecture Diagram:

```
STL Architecture:
[Input Text] â†’ [PhoBERT] â†’ [Classifier_texture] â†’ [Output_texture]
[Input Text] â†’ [PhoBERT] â†’ [Classifier_smell] â†’ [Output_smell]  
[Input Text] â†’ [PhoBERT] â†’ [Classifier_price] â†’ [Output_price]
... (N separate models)

MTL Architecture:
[Input Text] â†’ [Shared PhoBERT] â†’ [Classifier_texture] â†’ [Output_texture]
                                â†— [Classifier_smell] â†’ [Output_smell]
                                â†— [Classifier_price] â†’ [Output_price]
                                â†— ... (all aspects)
```

### ğŸ”§ Key Code Differences:

**STL Forward Pass:**
```python
def forward(self, input_ids, attention_mask):
    outputs = self.transformer(input_ids=input_ids, attention_mask=attention_mask)
    pooled_output = outputs.pooler_output
    output = self.dropout(pooled_output)
    logits = self.classifier(output)  # Single output
    return logits
```

**MTL Forward Pass:**
```python
def forward(self, input_ids, attention_mask):
    outputs = self.transformer(input_ids=input_ids, attention_mask=attention_mask)
    shared_features = self.dropout(outputs.pooler_output)  # Shared features
    
    logits = {}
    for aspect in self.classifiers:
        logits[aspect] = self.classifiers[aspect](shared_features)  # Multiple outputs
    return logits
```

## ğŸ› ï¸ CÃ´ng nghá»‡ sá»­ dá»¥ng

- **Model**: PhoBERT (vinai/phobert-base) - BERT cho tiáº¿ng Viá»‡t
- **Framework**: PyTorch, Transformers
- **Metrics**: Accuracy, F1-score (weighted)
- **Hardware**: GPU/CPU vá»›i tá»‘i Æ°u hÃ³a memory

## ğŸ“Š Domains vÃ  Aspects

### 1. ğŸ’„ Cosmetic Domain
- **stayingpower**: Äá»™ bá»n cá»§a sáº£n pháº©m
- **texture**: Cháº¥t cáº£m sáº£n pháº©m  
- **smell**: MÃ¹i hÆ°Æ¡ng
- **price**: GiÃ¡ cáº£
- **colour**: MÃ u sáº¯c
- **shipping**: Váº­n chuyá»ƒn
- **packing**: ÄÃ³ng gÃ³i

### 2. ğŸ¨ Hotel Domain  
- **FACILITIES**: Tiá»‡n nghi (cleanliness, comfort, design, general)
- **ROOMS**: PhÃ²ng á»Ÿ (cleanliness, comfort, design, general, quality)
- **SERVICE**: Dá»‹ch vá»¥ chung
- **LOCATION**: Vá»‹ trÃ­ Ä‘á»‹a lÃ½

### 3. ğŸ½ï¸ Restaurant Domain
- **FOOD**: Äá»“ Äƒn (prices, quality, style&options)
- **DRINKS**: Äá»“ uá»‘ng (prices, quality, style&options)  
- **AMBIENCE**: KhÃ´ng gian quÃ¡n
- **SERVICE**: Phá»¥c vá»¥

## ğŸš€ HÆ°á»›ng dáº«n cháº¡y

### 1. CÃ i Ä‘áº·t dependencies

```bash
pip install -r requirements.txt
```

### 2. Cháº¡y notebook

```bash
jupyter notebook "nlp-lab-10 (1).ipynb"
```

### 3. Thá»±c thi comparison

```python
# Cháº¡y so sÃ¡nh Ä‘áº§y Ä‘á»§
main()

# Hoáº·c phiÃªn báº£n nháº¹ cho memory háº¡n cháº¿
lightweight_comparison()
```

## ğŸ“ˆ Káº¿t quáº£

### Memory Optimization
- **Batch size**: 1-2 (tá»‘i Æ°u cho GPU memory)
- **Max length**: 96 tokens (giáº£m tá»« 256)
- **Gradient accumulation**: Effective batch size lá»›n hÆ¡n
- **CPU fallback**: Tá»± Ä‘á»™ng chuyá»ƒn sang CPU náº¿u GPU háº¿t memory

### Performance Metrics
Model Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡ báº±ng:
- **Accuracy**: Äá»™ chÃ­nh xÃ¡c tá»•ng thá»ƒ
- **F1-score**: F1 weighted cho multi-class

## ğŸ”§ Tá»‘i Æ°u hÃ³a Memory

Dá»± Ã¡n Ä‘Æ°á»£c tá»‘i Æ°u hÃ³a Ä‘á»ƒ cháº¡y trÃªn hardware háº¡n cháº¿:

```python
# Cáº¥u hÃ¬nh tá»‘i Æ°u memory
comparison = STL_vs_MTL_Comparison(
    model_name="vinai/phobert-base",
    batch_size=1,           # Giáº£m batch size
    max_length=96,          # Giáº£m sequence length  
    force_cpu=True          # DÃ¹ng CPU náº¿u cáº§n
)
```

## ğŸ“š Nguá»“n dá»¯ liá»‡u

- **VLSP 2018**: Vietnamese Language and Speech Processing Workshop
- **Cosmetic Dataset**: Dá»¯ liá»‡u review má»¹ pháº©m tiáº¿ng Viá»‡t

## ğŸ“ LiÃªn há»‡

- **TÃ¡c giáº£**: Pham Trung Truc
- **Email**: phamtruc120604@gamil.com
- **GitHub**: GenTpham

## ğŸ™ Acknowledgments

- [PhoBERT](https://github.com/VinAIResearch/PhoBERT) - Pre-trained BERT for Vietnamese
- [VLSP 2018](http://vlsp.org.vn/) - Vietnamese Language and Speech Processing Workshop
- [Transformers](https://huggingface.co/transformers/) - State-of-the-art NLP library 